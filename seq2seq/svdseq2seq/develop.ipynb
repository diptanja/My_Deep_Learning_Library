{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training data: ', (1000, 784), (1000, 10))\n",
      "('Test data: ', (100, 784), (100, 10))\n",
      "Running network:  svdMLP\n",
      "Input dimension:  784\n",
      "Output dimension:  10\n",
      "Hidden layers:  10\n",
      "Hidden dimension:  128\n",
      "Number of reflection vectors:  16\n",
      "Singualr margin:  0.1\n",
      "Hidden-to-hidden activation function:  leaky_relu\n",
      "Output activation function:  softmax\n",
      "Instance per batch:  1000\n",
      "Validation interval:  100\n",
      "Number of epoch:  10\n",
      "Learning rate:  0.0001\n",
      "Droput rate:  0.1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Import Self Defined Library Functions\n",
    "\n",
    "import load \n",
    "import custom_config \n",
    "import activations\n",
    "import utils \n",
    "import debug\n",
    "\n",
    "## Import Library Function\n",
    "\n",
    "import numpy as np\n",
    "#import theano\n",
    "# from theano import function, config, shared, tensor\n",
    "import time\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('/Users/achanish/anaconda/lib/python2.7/site-packages')\n",
    "\n",
    "import mxnet as mx\n",
    "\n",
    "## Load Data ##\n",
    "\n",
    "trX, teX, trY, teY = load.mnist(ntrain=1000, ntest=100)\n",
    "print (\"Training data: \", trX.shape, trY.shape)\n",
    "print (\"Test data: \", teX.shape, teY.shape)\n",
    "print debug.print_setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, predict_input = None, W=None, b=None,\n",
    "                 activation=activations.hyperbolic_tangent, dropout_rate = 0, nametag=''):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type rng: numpy.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: theano.Op or function\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        if predict_input is None:\n",
    "            predict_input = input\n",
    "        self.input = input\n",
    "        self.predict_input = predict_input\n",
    "        # end-snippet-1\n",
    "\n",
    "        # `W` is initialized with `W_values` which is uniformely sampled\n",
    "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
    "        # for tanh activation function\n",
    "        # the output of uniform if converted using asarray to dtype\n",
    "        # theano.config.floatX so that the code is runable on GPU\n",
    "        # Note : optimal initialization of weights is dependent on the\n",
    "        #        activation function used (among other things).\n",
    "        #        For example, results presented in [Xavier10] suggest that you\n",
    "        #        should use 4 times larger initial weights for sigmoid\n",
    "        #        compared to tanh\n",
    "        #        We have no info for other function, so we use the same as\n",
    "        #        tanh.\n",
    "        if W is None:\n",
    "            W_values = np.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                    high=np.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                \n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, name='W'+nametag, borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b'+nametag, borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        output = activation(T.dot(input, self.W) + self.b)\n",
    "        predict_output = activation(T.dot(predict_input, self.W) + self.b)\n",
    "        if dropout_rate > 0:\n",
    "            #np.random.seed(int(time.time()))\n",
    "            #mask = np.random.binomial(np.ones(n_out, dtype=int),1.0-dropout_rate)\n",
    "            srng = RandomStreams(rng.randint(999999))\n",
    "            mask = srng.binomial(n=1, p=1.0-dropout_rate, size=output.shape, dtype=theano.config.floatX)\n",
    "            self.output = output * mask\n",
    "            self.predict_output = (1.0-dropout_rate) * predict_output\n",
    "        else:\n",
    "            self.output = output\n",
    "            self.predict_output = predict_output\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
