{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 10000\n",
      "Vectorization...\n",
      "Training Data: (9000, 7, 12) (9000, 4, 12)\n",
      "Validation Data: (1000, 7, 12) (1000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "\n",
    "\n",
    "### Create Index Table For Encoding also define One-Hot Encoding  #####\n",
    "#######################################################################\n",
    "\n",
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "    \n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'        \n",
    "\n",
    "    \n",
    "#####   Define Parameters for the model and dataset.  #####\n",
    "###########################################################\n",
    "\n",
    "TRAINING_SIZE = 10000\n",
    "DIGITS = 3\n",
    "INVERT = True\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "\n",
    "\n",
    "##### Generate Sample Data For Addition Task ######\n",
    "###################################################\n",
    "\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if INVERT:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "\n",
    "\n",
    "#####  Encode the Data into One Hot Vector Format               ####\n",
    "#####  If N Examples and Max Len of i/p |Wi|-> 2L+1 (3+1+3 =7)  ####\n",
    "#####  |V| = 12 (0-9, ,+) So, N*(|Wi|*|V|) to store encoding X  ####\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    #print(i,sentence)\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "#print(x[0])\n",
    "    \n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "    #print(i,sentence)\n",
    "#print(y[3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Shuffle And Split Data ######\n",
    "####################################\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - (len(x)/10)\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:', x_train.shape, y_train.shape)\n",
    "print('Validation Data:',x_val.shape,y_val.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Seq2SEQ - LSTM UNITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 4, 12)             1548      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4, 12)             0         \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 7s - loss: 2.0886 - acc: 0.2799 - val_loss: 1.8297 - val_acc: 0.3640\n",
      "Q 35+424  T 459  \u001b[91m☒\u001b[0m 11  \n",
      "Q 59+696  T 755  \u001b[91m☒\u001b[0m 110 \n",
      "Q 54+722  T 776  \u001b[91m☒\u001b[0m 110 \n",
      "Q 487+80  T 567  \u001b[91m☒\u001b[0m 110 \n",
      "Q 9+952   T 961  \u001b[91m☒\u001b[0m 110 \n",
      "Q 16+849  T 865  \u001b[91m☒\u001b[0m 110 \n",
      "Q 45+6    T 51   \u001b[91m☒\u001b[0m 11  \n",
      "Q 3+692   T 695  \u001b[91m☒\u001b[0m 11  \n",
      "Q 27+3    T 30   \u001b[91m☒\u001b[0m 11  \n",
      "Q 58+876  T 934  \u001b[91m☒\u001b[0m 110 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 4s - loss: 1.7800 - acc: 0.3641 - val_loss: 1.7341 - val_acc: 0.3675\n",
      "Q 9+207   T 216  \u001b[91m☒\u001b[0m 144 \n",
      "Q 696+112 T 808  \u001b[91m☒\u001b[0m 110 \n",
      "Q 561+377 T 938  \u001b[91m☒\u001b[0m 110 \n",
      "Q 8+1     T 9    \u001b[91m☒\u001b[0m 14  \n",
      "Q 6+986   T 992  \u001b[91m☒\u001b[0m 114 \n",
      "Q 6+265   T 271  \u001b[91m☒\u001b[0m 14  \n",
      "Q 802+36  T 838  \u001b[91m☒\u001b[0m 110 \n",
      "Q 2+368   T 370  \u001b[91m☒\u001b[0m 14  \n",
      "Q 513+77  T 590  \u001b[91m☒\u001b[0m 114 \n",
      "Q 62+660  T 722  \u001b[91m☒\u001b[0m 110 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 4s - loss: 1.7510 - acc: 0.3668 - val_loss: 1.7218 - val_acc: 0.3693\n",
      "Q 173+733 T 906  \u001b[91m☒\u001b[0m 117 \n",
      "Q 27+3    T 30   \u001b[91m☒\u001b[0m 18  \n",
      "Q 747+6   T 753  \u001b[91m☒\u001b[0m 187 \n",
      "Q 58+876  T 934  \u001b[91m☒\u001b[0m 110 \n",
      "Q 430+4   T 434  \u001b[91m☒\u001b[0m 13  \n",
      "Q 160+28  T 188  \u001b[91m☒\u001b[0m 117 \n",
      "Q 86+103  T 189  \u001b[91m☒\u001b[0m 117 \n",
      "Q 747+6   T 753  \u001b[91m☒\u001b[0m 187 \n",
      "Q 1+76    T 77   \u001b[91m☒\u001b[0m 18  \n",
      "Q 4+778   T 782  \u001b[91m☒\u001b[0m 187 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 4s - loss: 1.7296 - acc: 0.3709 - val_loss: 1.7056 - val_acc: 0.3752\n",
      "Q 88+1    T 89   \u001b[91m☒\u001b[0m 18  \n",
      "Q 0+3     T 3    \u001b[91m☒\u001b[0m 4   \n",
      "Q 6+333   T 339  \u001b[91m☒\u001b[0m 13  \n",
      "Q 918+9   T 927  \u001b[91m☒\u001b[0m 100 \n",
      "Q 523+13  T 536  \u001b[91m☒\u001b[0m 133 \n",
      "Q 579+6   T 585  \u001b[91m☒\u001b[0m 100 \n",
      "Q 776+142 T 918  \u001b[91m☒\u001b[0m 110 \n",
      "Q 1+696   T 697  \u001b[91m☒\u001b[0m 100 \n",
      "Q 607+93  T 700  \u001b[91m☒\u001b[0m 100 \n",
      "Q 543+80  T 623  \u001b[91m☒\u001b[0m 103 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 4s - loss: 1.7153 - acc: 0.3736 - val_loss: 1.7049 - val_acc: 0.3732\n",
      "Q 982+70  T 1052 \u001b[91m☒\u001b[0m 100 \n",
      "Q 174+237 T 411  \u001b[91m☒\u001b[0m 113 \n",
      "Q 180+40  T 220  \u001b[91m☒\u001b[0m 101 \n",
      "Q 196+0   T 196  \u001b[91m☒\u001b[0m 101 \n",
      "Q 3+202   T 205  \u001b[91m☒\u001b[0m 333 \n",
      "Q 261+88  T 349  \u001b[91m☒\u001b[0m 101 \n",
      "Q 635+70  T 705  \u001b[91m☒\u001b[0m 103 \n",
      "Q 710+34  T 744  \u001b[91m☒\u001b[0m 103 \n",
      "Q 200+2   T 202  \u001b[91m☒\u001b[0m 133 \n",
      "Q 1+52    T 53   \u001b[91m☒\u001b[0m 12  \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 4s - loss: 1.6940 - acc: 0.3821 - val_loss: 1.6667 - val_acc: 0.3820\n",
      "Q 0+133   T 133  \u001b[92m☑\u001b[0m 133 \n",
      "Q 100+19  T 119  \u001b[91m☒\u001b[0m 111 \n",
      "Q 5+932   T 937  \u001b[91m☒\u001b[0m 136 \n",
      "Q 4+362   T 366  \u001b[91m☒\u001b[0m 434 \n",
      "Q 583+4   T 587  \u001b[91m☒\u001b[0m 154 \n",
      "Q 7+376   T 383  \u001b[91m☒\u001b[0m 176 \n",
      "Q 353+305 T 658  \u001b[91m☒\u001b[0m 133 \n",
      "Q 679+474 T 1153 \u001b[91m☒\u001b[0m 1156\n",
      "Q 5+741   T 746  \u001b[91m☒\u001b[0m 156 \n",
      "Q 0+84    T 84   \u001b[91m☒\u001b[0m 17  \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 4s - loss: 1.6648 - acc: 0.3877 - val_loss: 1.6317 - val_acc: 0.3898\n",
      "Q 86+988  T 1074 \u001b[91m☒\u001b[0m 101 \n",
      "Q 224+98  T 322  \u001b[91m☒\u001b[0m 129 \n",
      "Q 8+38    T 46   \u001b[91m☒\u001b[0m 11  \n",
      "Q 2+37    T 39   \u001b[91m☒\u001b[0m 21  \n",
      "Q 45+6    T 51   \u001b[91m☒\u001b[0m 41  \n",
      "Q 891+63  T 954  \u001b[91m☒\u001b[0m 101 \n",
      "Q 41+778  T 819  \u001b[91m☒\u001b[0m 111 \n",
      "Q 579+6   T 585  \u001b[91m☒\u001b[0m 101 \n",
      "Q 2+552   T 554  \u001b[91m☒\u001b[0m 420 \n",
      "Q 847+17  T 864  \u001b[91m☒\u001b[0m 111 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 4s - loss: 1.6278 - acc: 0.3954 - val_loss: 1.6168 - val_acc: 0.4025\n",
      "Q 67+67   T 134  \u001b[91m☒\u001b[0m 776 \n",
      "Q 8+845   T 853  \u001b[91m☒\u001b[0m 144 \n",
      "Q 76+253  T 329  \u001b[91m☒\u001b[0m 166 \n",
      "Q 2+933   T 935  \u001b[91m☒\u001b[0m 330 \n",
      "Q 86+151  T 237  \u001b[91m☒\u001b[0m 116 \n",
      "Q 430+4   T 434  \u001b[91m☒\u001b[0m 447 \n",
      "Q 872+384 T 1256 \u001b[91m☒\u001b[0m 1113\n",
      "Q 7+439   T 446  \u001b[91m☒\u001b[0m 144 \n",
      "Q 5+322   T 327  \u001b[91m☒\u001b[0m 336 \n",
      "Q 67+508  T 575  \u001b[91m☒\u001b[0m 176 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 4s - loss: 1.5959 - acc: 0.4056 - val_loss: 1.5884 - val_acc: 0.4145\n",
      "Q 8+423   T 431  \u001b[91m☒\u001b[0m 449 \n",
      "Q 4+963   T 967  \u001b[91m☒\u001b[0m 909 \n",
      "Q 738+769 T 1507 \u001b[91m☒\u001b[0m 1113\n",
      "Q 7+185   T 192  \u001b[91m☒\u001b[0m 182 \n",
      "Q 74+53   T 127  \u001b[91m☒\u001b[0m 540 \n",
      "Q 790+60  T 850  \u001b[91m☒\u001b[0m 106 \n",
      "Q 542+88  T 630  \u001b[91m☒\u001b[0m 555 \n",
      "Q 506+997 T 1503 \u001b[91m☒\u001b[0m 1013\n",
      "Q 390+9   T 399  \u001b[91m☒\u001b[0m 990 \n",
      "Q 901+74  T 975  \u001b[91m☒\u001b[0m 109 \n"
     ]
    }
   ],
   "source": [
    "# Try replacing GRU, or SimpleRNN.\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "print('Build model...')\n",
    "\n",
    "\n",
    "###### ENCODER ####\n",
    "\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model = Sequential()\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####### DECODER ######\n",
    "\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "    \n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "\n",
    "#### TODO- CUSTOM OPTIMIZER ###########\n",
    "#####   ***********************  ######\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for iteration in range(1, 10):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if INVERT else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
